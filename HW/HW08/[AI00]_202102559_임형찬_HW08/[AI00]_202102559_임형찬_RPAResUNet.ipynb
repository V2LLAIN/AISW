{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d53fc047-7b87-4817-90ac-36c91ee00ba5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import cv2\n",
    "import wandb\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_metric\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "from torchmetrics.classification import BinaryJaccardIndex\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from glob import glob\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b37c70a-f5d8-40e8-a3e8-f858a7229c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class segDataset(Dataset):\n",
    "    def __init__(self, data_path, transforms=None):\n",
    "        # cata path 설정 잘하기\n",
    "        self.pos_imgs = sorted(glob(data_path + 'Positive/Image/*'))\n",
    "        self.pos_labels = sorted(glob(data_path + 'Positive/Label/*'))\n",
    "        self.neg_imgs = sorted(glob(data_path + 'Negative/Image/*'))\n",
    "        self.neg_labels = sorted(glob(data_path + 'Negative/Label /*'))\n",
    "        # positive와 negative를 합쳐서 불러오는 코드를 작성\n",
    "        self.imgs = self.pos_imgs + self.neg_imgs\n",
    "        self.labels = self.pos_labels + self.neg_labels\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        img_path = self.imgs[item]\n",
    "        label_path = self.labels[item]\n",
    "        img = cv2.imread(img_path)\n",
    "        label = cv2.imread(label_path, cv2.IMREAD_UNCHANGED)\n",
    "        label = np.expand_dims(label, axis=2)\n",
    "\n",
    "        concat = np.concatenate([img, label], axis=2)\n",
    "        concat = torch.from_numpy(concat)\n",
    "        concat = concat.permute(2,0,1) # (h,w,c -> c,h,w)\n",
    "\n",
    "        \n",
    "        if self.transforms:\n",
    "            imgs = self.transforms(concat)\n",
    "\n",
    "        X = imgs[:3].to(torch.float32)\n",
    "        y = imgs[3].to(torch.float32)\n",
    "            \n",
    "        return {'X' : X/255, 'y': y/255}\n",
    "\n",
    "\n",
    "\n",
    "data_path = './data/Train/'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # transforms.ToTensor(), # (h,w,c -> c,h,w) + Normalize\n",
    "])\n",
    "\n",
    "training_data = segDataset(data_path=data_path, transforms=transform)\n",
    "\n",
    "total_samples = len(training_data)\n",
    "train_size = int(0.8 * total_samples)\n",
    "val_size = total_samples - train_size\n",
    "\n",
    "# 인덱스를 무작위로 섞음\n",
    "indices = list(range(total_samples))\n",
    "np.random.shuffle(indices)\n",
    "train_sampler = SubsetRandomSampler(indices[:train_size])\n",
    "val_sampler = SubsetRandomSampler(indices[train_size:])\n",
    "\n",
    "\n",
    "# DataLoader 설정\n",
    "train_dataloader = DataLoader(training_data, batch_size=16, sampler=train_sampler)\n",
    "val_dataloader = DataLoader(training_data, batch_size=16, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a809894-9e90-49cf-a12b-e3fc8ffd4073",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_dims, out_dims, rate=[6, 12, 18]):\n",
    "        super(ASPP, self).__init__()\n",
    "        \n",
    "        self.aspp_block1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_dims, out_dims, 3, stride=1, padding=rate[0], dilation=rate[0]\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_dims),\n",
    "        )\n",
    "        self.aspp_block2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_dims, out_dims, 3, stride=1, padding=rate[1], dilation=rate[1]\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_dims),\n",
    "        )\n",
    "        self.aspp_block3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_dims, out_dims, 3, stride=1, padding=rate[2], dilation=rate[2]\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_dims),\n",
    "        )\n",
    "\n",
    "        self.output = nn.Conv2d(len(rate) * out_dims, out_dims, 1)\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.aspp_block1(x)\n",
    "        x2 = self.aspp_block2(x)\n",
    "        x3 = self.aspp_block3(x)\n",
    "        out = torch.cat([x1, x2, x3], dim=1)\n",
    "        return self.output(out)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x = torch.cat([avg_out, max_out], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        return self.sigmoid(x)\n",
    "                \n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "           \n",
    "        self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // 16, 1, bias=False),\n",
    "                               nn.ReLU(),\n",
    "                               nn.Conv2d(in_planes // 16, in_planes, 1, bias=False))\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc(self.avg_pool(x))\n",
    "        max_out = self.fc(self.max_pool(x))\n",
    "        out = avg_out + max_out\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, padding):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1),\n",
    "        ) \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        r = self.conv_block(inputs)\n",
    "        s = self.shortcut(inputs)\n",
    "        \n",
    "        skip = r + s\n",
    "        return skip\n",
    "    \n",
    "class RPAResUNet(pl.LightningModule):\n",
    "    def __init__(self, num_classes):\n",
    "        super(RPAResUNet, self).__init__()\n",
    "        self.jaccard = BinaryJaccardIndex()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        \"\"\" Input block -> residual layer \"\"\"\n",
    "        self.residual_1_a = self.input_block(in_channels=3, out_channels=32)\n",
    "        self.residual_1_b = self.input_skip(in_channels=3, out_channels=32)\n",
    "        # input block -> spatial attention\n",
    "        self.spatial_attention = SpatialAttention()\n",
    "        \n",
    "        \"\"\" Residual block x 3, Encoding \"\"\"\n",
    "        self.residual_2 = ResidualBlock(32, 64, 2, 1)\n",
    "        self.residual_3 = ResidualBlock(64, 128, 2, 1)\n",
    "        self.residual_4 = ResidualBlock(128, 256, 2, 1)\n",
    "        \n",
    "        \"\"\" Bridge block, ASPP \"\"\"\n",
    "        # 3 -> 32, 1024 -> 512, computation cost\n",
    "        self.bridge_aspp = ASPP(256, 512)\n",
    "        \n",
    "        \"\"\" Last Encoder layer <- channel attention \"\"\"\n",
    "        self.channel_attention = ChannelAttention(512)\n",
    "        \n",
    "        \"\"\" Residual block x3, Decoding 1st block \"\"\"\n",
    "        # upsample out_feature 512 -> 256 ?\n",
    "        self.upsample_block_1 = nn.ConvTranspose2d(in_channels=512, out_channels=512, \n",
    "                                          kernel_size=2, stride=2, padding=0)\n",
    "        self.residual_5 = ResidualBlock(512+128, 256, 1, 1) # upsampling + residual_3\n",
    "        \n",
    "        \"\"\" 2nd Residual block, Decoder \"\"\"\n",
    "        self.upsample_block_2 = nn.ConvTranspose2d(in_channels=256, out_channels=256, \n",
    "                                          kernel_size=2, stride=2, padding=0)\n",
    "        self.residual_6 = ResidualBlock(256+64, 128, 1, 1)\n",
    "        \n",
    "        \"\"\" 3rd Residual block, Decoder \"\"\"\n",
    "        self.upsample_block_3 = nn.ConvTranspose2d(in_channels=128, out_channels=128, \n",
    "                                          kernel_size=2, stride=2, padding=0)\n",
    "        self.residual_7 = ResidualBlock(128+32, 64, 1, 1)\n",
    "        \n",
    "        \"\"\" output block \"\"\"\n",
    "        self.output_aspp = ASPP(64, 32)\n",
    "        self.output = nn.Sequential(\n",
    "          nn.Conv2d(in_channels=32, out_channels=num_classes, kernel_size=1),\n",
    "          nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def input_block(self, in_channels, out_channels):\n",
    "        block = nn.Sequential(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                                    nn.BatchNorm2d(num_features=out_channels),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Conv2d(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                                    )\n",
    "        return block\n",
    "    \n",
    "    def input_skip(self, in_channels, out_channels):\n",
    "        skip = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        return skip \n",
    "\n",
    "    def forward(self, X):\n",
    "        residual_1_a_out = self.residual_1_a(X)\n",
    "        residual_1_b_out = self.residual_1_b(X)\n",
    "        residual_1_out = residual_1_a_out + residual_1_b_out\n",
    "        spatial_attention_out = self.spatial_attention(residual_1_out) * residual_1_out\n",
    "        \n",
    "        residual_2_out = self.residual_2(spatial_attention_out)\n",
    "        residual_3_out = self.residual_3(residual_2_out)\n",
    "        residual_4_out = self.residual_4(residual_3_out)\n",
    "        \n",
    "        bridge_aspp_a = self.bridge_aspp(residual_4_out)\n",
    "        channel_attention_out = self.channel_attention(bridge_aspp_a) * bridge_aspp_a\n",
    "        \n",
    "        \"\"\" decoder block without attention modules \"\"\"\n",
    "        \n",
    "        upsample_1_out = self.upsample_block_1(channel_attention_out)\n",
    "        residual_5_out = self.residual_5(torch.cat((upsample_1_out, residual_3_out), dim=1))\n",
    "        \n",
    "        upsample_2_out = self.upsample_block_2(residual_5_out)\n",
    "        residual_6_out = self.residual_6(torch.cat((upsample_2_out, residual_2_out), dim=1))\n",
    "        \n",
    "        upsample_3_out = self.upsample_block_3(residual_6_out)\n",
    "        residual_7_out = self.residual_7(torch.cat((upsample_3_out, residual_1_out), dim=1))\n",
    "        \n",
    "        output_aspp_b = self.output_aspp(residual_7_out)\n",
    "        out = self.output(output_aspp_b)\n",
    "        return out\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                               mode='min',\t# Loss최소화,최대화 결정\n",
    "                                                               factor=1e-8,\t# 학습률 감소율\n",
    "                                                               patience=5,\n",
    "                                                               verbose=True,)\n",
    "        monitor_metric = 'val_loss'\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': monitor_metric  # Specify the metric you want to monitor\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch['X'], batch['y']\n",
    "        outputs = self(x).squeeze(dim=1)\n",
    "        loss = nn.BCELoss()(outputs, y)\n",
    "        \n",
    "        predicted_masks = (outputs > 0.5).to(torch.uint8) # 0아니면 1로 바꾸는\n",
    "        y = y.to(torch.uint8)\n",
    "        \n",
    "        iou = self.jaccard(predicted_masks, y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        self.log('train_iou', iou, on_step=True, on_epoch=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch['X'], batch['y']\n",
    "        outputs = self(x).squeeze(dim=1)\n",
    "        loss = nn.BCELoss()(outputs, y)\n",
    "        \n",
    "        predicted_masks = (outputs > 0.5).to(torch.uint8)\n",
    "        y = y.to(torch.uint8)\n",
    "  \n",
    "        iou = self.jaccard(predicted_masks, y)\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_iou', iou, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0454dd6a-dd09-4839-9f63-9ca3db608e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchan4im\u001b[0m (\u001b[33mhcim\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX A6000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20231109_193140-5t4kxe1e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hcim/AISW/runs/5t4kxe1e' target=\"_blank\">RPAResUNet</a></strong> to <a href='https://wandb.ai/hcim/AISW' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hcim/AISW' target=\"_blank\">https://wandb.ai/hcim/AISW</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hcim/AISW/runs/5t4kxe1e' target=\"_blank\">https://wandb.ai/hcim/AISW/runs/5t4kxe1e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "   | Name              | Type               | Params\n",
      "----------------------------------------------------------\n",
      "0  | jaccard           | BinaryJaccardIndex | 0     \n",
      "1  | residual_1_a      | Sequential         | 10.2 K\n",
      "2  | residual_1_b      | Conv2d             | 896   \n",
      "3  | spatial_attention | SpatialAttention   | 98    \n",
      "4  | residual_2        | ResidualBlock      | 74.1 K\n",
      "5  | residual_3        | ResidualBlock      | 295 K \n",
      "6  | residual_4        | ResidualBlock      | 1.2 M \n",
      "7  | bridge_aspp       | ASPP               | 4.3 M \n",
      "8  | channel_attention | ChannelAttention   | 32.8 K\n",
      "9  | upsample_block_1  | ConvTranspose2d    | 1.0 M \n",
      "10 | residual_5        | ResidualBlock      | 3.5 M \n",
      "11 | upsample_block_2  | ConvTranspose2d    | 262 K \n",
      "12 | residual_6        | ResidualBlock      | 886 K \n",
      "13 | upsample_block_3  | ConvTranspose2d    | 65.7 K\n",
      "14 | residual_7        | ResidualBlock      | 221 K \n",
      "15 | output_aspp       | ASPP               | 58.7 K\n",
      "16 | output            | Sequential         | 33    \n",
      "----------------------------------------------------------\n",
      "12.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "12.0 M    Total params\n",
      "48.043    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9933c6ed98b74a55a1ed1cc3cf2056f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.328\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.218 >= min_delta = 0.0. New best score: 0.110\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.017 >= min_delta = 0.0. New best score: 0.092\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.008 >= min_delta = 0.0. New best score: 0.085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.084\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5523951cdf5a4bc18feca043d2ffea66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# WandbLogger를 초기화\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "wandb.login(key='eed81e1c0a41dd8dd67a4ca90cea1be5a06d4eb0')\n",
    "wandb_logger = WandbLogger(project='AISW', entity='hcim', name='RPAResUNet')\n",
    "\n",
    "model = RPAResUNet(num_classes=1)\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=8,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(devices=[0],max_epochs=10, logger=wandb_logger, callbacks=[early_stopping_callback])\n",
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ade64b-5f57-4fd4-b570-9ac0e999f58e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da3772e-2933-49a9-896f-1a63037e4206",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class segDataset(Dataset):\n",
    "    def __init__(self, data_path, transforms=None):\n",
    "        # cata path 설정 잘하기\n",
    "        self.pos_imgs = sorted(glob(data_path + 'Positive/Image/*'))\n",
    "        self.pos_labels = sorted(glob(data_path + 'Positive/Label/*'))\n",
    "        self.neg_imgs = sorted(glob(data_path + 'Negative/Image/*'))\n",
    "        self.neg_labels = sorted(glob(data_path + 'Negative/Label /*'))\n",
    "        # positive와 negative를 합쳐서 불러오는 코드를 작성\n",
    "        self.imgs = self.pos_imgs + self.neg_imgs\n",
    "        self.labels = self.pos_labels + self.neg_labels\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        img_path = self.imgs[item]\n",
    "        label_path = self.labels[item]\n",
    "        img = cv2.imread(img_path)\n",
    "        label = cv2.imread(label_path, cv2.IMREAD_UNCHANGED)\n",
    "        label = np.expand_dims(label, axis=2)\n",
    "\n",
    "        concat = np.concatenate([img, label], axis=2)\n",
    "        concat = torch.from_numpy(concat)\n",
    "        concat = concat.permute(2,0,1) # (h,w,c -> c,h,w)\n",
    "\n",
    "        \n",
    "        if self.transforms:\n",
    "            imgs = self.transforms(concat)\n",
    "\n",
    "        X = imgs[:3].to(torch.float32)\n",
    "        y = imgs[3].to(torch.float32)\n",
    "            \n",
    "        return {'X' : X/255, 'y': y/255}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a64f9-830a-4710-9c55-a77a8d8888c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "data_path = './data/Train/'\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "set_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f9c60c-e94a-4bec-be26-574e44446e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor.squeeze()\n",
    "    numpy_image = tensor.cpu().numpy()\n",
    "    numpy_image = (numpy_image * 255).astype(np.uint8)\n",
    "    \n",
    "    if numpy_image.shape[0] == 1: # 흑백 이미지라면 채널 차원을 제거.\n",
    "        numpy_image = np.squeeze(numpy_image, axis=0)\n",
    "        \n",
    "    elif numpy_image.shape[0] == 3: # RGB 이미지라면 채널 차원을 맨 뒤로 이동.\n",
    "        numpy_image = np.transpose(numpy_image, (1, 2, 0))\n",
    "    return numpy_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cbdf2c-5b2c-4f25-9300-2437bac6ed9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "model.eval()\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(), # (h,w,c -> c,h,w) + Normalize\n",
    "])\n",
    "def inference_and_save_images(input_dir, output_dir, start_index, end_index):\n",
    "    for i in range(start_index, end_index + 1):\n",
    "        input_path = os.path.join(input_dir, f\"{i}.jpg\")\n",
    "        output_path = os.path.join(output_dir, f\"{i}.jpg\")\n",
    "\n",
    "        # 파일이 존재하지 않으면 스킵. Test/Positive/8322.jpg가 없음\n",
    "        if not os.path.exists(input_path):\n",
    "            print(f\"File {input_path} not found. Skipping...\")\n",
    "            continue\n",
    "            \n",
    "        # 이미지 불러오기 및 전처리\n",
    "        img = Image.open(input_path)\n",
    "        img = transform(img).unsqueeze(0)\n",
    "\n",
    "        # 추론\n",
    "        with torch.no_grad():\n",
    "            output = model(img)\n",
    "\n",
    "        # 텐서를 이미지로 변환후 저장\n",
    "        output_image = tensor_to_image(output)\n",
    "        cv2.imwrite(output_path, output_image)\n",
    "\n",
    "        print(f\"Inferred and saved {input_path} to {output_path}\")\n",
    "\n",
    "# Positive 예측\n",
    "positive_input_dir = \"./data/Test/Positive/\"\n",
    "positive_output_dir = \"./data/Prediction/Positive/\"\n",
    "inference_and_save_images(positive_input_dir, positive_output_dir, 8000, 8817)\n",
    "\n",
    "# Negative 예측\n",
    "negative_input_dir = \"./data/Test/Negative/\"\n",
    "negative_output_dir = \"./data/Prediction/Negative/\"\n",
    "inference_and_save_images(negative_input_dir, negative_output_dir, 1001, 1181)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2cbb3-b0ff-4469-9dfe-8a646ef689d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ffab85-e4ef-40e5-964e-077ef73110de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!zip -r Prediction.zip ./data/Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5c03fd-5e4b-4693-a737-8b3fcd73e9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for i in range(x.shape[0]):\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    cv2.imwrite(f\"{batch_idx}_img.jpg\", x[i].cpu().numpy()*255)\n",
    "    cv2.imwrite(f\"{batch_idx}_label.jpg\", y[i].cpu().numpy()*255)\n",
    "    cv2.imwrite(f\"{batch_idx}_output.jpg\", outputs[i].cpu().detach().numpy()*255) \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c971b45-1f1e-4c44-b158-315c80f60623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
