{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d53fc047-7b87-4817-90ac-36c91ee00ba5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import cv2\n",
    "import wandb\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_metric\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "from torchmetrics.classification import BinaryJaccardIndex\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from glob import glob\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b37c70a-f5d8-40e8-a3e8-f858a7229c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class segDataset(Dataset):\n",
    "    def __init__(self, data_path, transforms=None):\n",
    "        # cata path 설정 잘하기\n",
    "        self.pos_imgs = sorted(glob(data_path + 'Positive/Image/*'))\n",
    "        self.pos_labels = sorted(glob(data_path + 'Positive/Label/*'))\n",
    "        self.neg_imgs = sorted(glob(data_path + 'Negative/Image/*'))\n",
    "        self.neg_labels = sorted(glob(data_path + 'Negative/Label /*'))\n",
    "        # positive와 negative를 합쳐서 불러오는 코드를 작성\n",
    "        self.imgs = self.pos_imgs + self.neg_imgs\n",
    "        self.labels = self.pos_labels + self.neg_labels\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        img_path = self.imgs[item]\n",
    "        label_path = self.labels[item]\n",
    "        img = cv2.imread(img_path)\n",
    "        label = cv2.imread(label_path, cv2.IMREAD_UNCHANGED)\n",
    "        label = np.expand_dims(label, axis=2)\n",
    "\n",
    "        concat = np.concatenate([img, label], axis=2)\n",
    "        concat = torch.from_numpy(concat)\n",
    "        concat = concat.permute(2,0,1) # (h,w,c -> c,h,w)\n",
    "\n",
    "        \n",
    "        if self.transforms:\n",
    "            imgs = self.transforms(concat)\n",
    "\n",
    "        X = imgs[:3].to(torch.float32)\n",
    "        y = imgs[3].to(torch.float32)\n",
    "            \n",
    "        return {'X' : X/255, 'y': y/255}\n",
    "\n",
    "\n",
    "\n",
    "data_path = './data/Train/'\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # transforms.ToTensor(), # (h,w,c -> c,h,w) + Normalize\n",
    "])\n",
    "\n",
    "training_data = segDataset(data_path=data_path, transforms=transform)\n",
    "\n",
    "total_samples = len(training_data)\n",
    "train_size = int(0.8 * total_samples)\n",
    "val_size = total_samples - train_size\n",
    "\n",
    "# 인덱스를 무작위로 섞음\n",
    "indices = list(range(total_samples))\n",
    "np.random.shuffle(indices)\n",
    "train_sampler = SubsetRandomSampler(indices[:train_size])\n",
    "val_sampler = SubsetRandomSampler(indices[train_size:])\n",
    "\n",
    "\n",
    "# DataLoader 설정\n",
    "train_dataloader = DataLoader(training_data, batch_size=16, sampler=train_sampler)\n",
    "val_dataloader = DataLoader(training_data, batch_size=16, sampler=val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6463b54-df80-4fdb-aee0-1aecca25fd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from modeling.sync_batchnorm.batchnorm import SynchronizedBatchNorm2d\n",
    "from modeling.aspp import build_aspp\n",
    "from modeling.decoder import build_decoder\n",
    "from modeling.backbone import build_backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54c36149-78fb-47c5-82fc-3d3d5f57c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "def init_weights(net, init_type='normal', gain=0.02):\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            if init_type == 'normal':\n",
    "                init.normal_(m.weight.data, 0.0, gain)\n",
    "            elif init_type == 'xavier':\n",
    "                init.xavier_normal_(m.weight.data, gain=gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                init.orthogonal_(m.weight.data, gain=gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find('BatchNorm2d') != -1:\n",
    "            init.normal_(m.weight.data, 1.0, gain)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    print('initialize network with %s' % init_type)\n",
    "    net.apply(init_func)\n",
    "\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(conv_block,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(up_conv,self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1,bias=True),\n",
    "\t\t    nn.BatchNorm2d(ch_out),\n",
    "\t\t\tnn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "class Recurrent_block(nn.Module):\n",
    "    def __init__(self,ch_out,t=2):\n",
    "        super(Recurrent_block,self).__init__()\n",
    "        self.t = t\n",
    "        self.ch_out = ch_out\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_out,ch_out,kernel_size=3,stride=1,padding=1,bias=True),\n",
    "\t\t    nn.BatchNorm2d(ch_out),\n",
    "\t\t\tnn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        for i in range(self.t):\n",
    "\n",
    "            if i==0:\n",
    "                x1 = self.conv(x)\n",
    "            \n",
    "            x1 = self.conv(x+x1)\n",
    "        return x1\n",
    "        \n",
    "class RRCNN_block(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out,t=2):\n",
    "        super(RRCNN_block,self).__init__()\n",
    "        self.RCNN = nn.Sequential(\n",
    "            Recurrent_block(ch_out,t=t),\n",
    "            Recurrent_block(ch_out,t=t)\n",
    "        )\n",
    "        self.Conv_1x1 = nn.Conv2d(ch_in,ch_out,kernel_size=1,stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.Conv_1x1(x)\n",
    "        x1 = self.RCNN(x)\n",
    "        return x+x1\n",
    "\n",
    "\n",
    "class single_conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(single_conv,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class Attention_block(nn.Module):\n",
    "    def __init__(self,F_g,F_l,F_int):\n",
    "        super(Attention_block,self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "            )\n",
    "        \n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self,g,x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1+x1)\n",
    "        psi = self.psi(psi)\n",
    "\n",
    "        return x*psi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a809894-9e90-49cf-a12b-e3fc8ffd4073",
   "metadata": {},
   "outputs": [],
   "source": [
    "class U_Net(pl.LightningModule):\n",
    "    def __init__(self,img_ch=3,output_ch=1):\n",
    "        super(U_Net,self).__init__()\n",
    "        self.jaccard = BinaryJaccardIndex()\n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(ch_in=img_ch,ch_out=64)\n",
    "        self.Conv2 = conv_block(ch_in=64,ch_out=128)\n",
    "        self.Conv3 = conv_block(ch_in=128,ch_out=256)\n",
    "        self.Conv4 = conv_block(ch_in=256,ch_out=512)\n",
    "        self.Conv5 = conv_block(ch_in=512,ch_out=1024)\n",
    "\n",
    "        self.Up5 = up_conv(ch_in=1024,ch_out=512)\n",
    "        self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)\n",
    "\n",
    "        self.Up4 = up_conv(ch_in=512,ch_out=256)\n",
    "        self.Up_conv4 = conv_block(ch_in=512, ch_out=256)\n",
    "        \n",
    "        self.Up3 = up_conv(ch_in=256,ch_out=128)\n",
    "        self.Up_conv3 = conv_block(ch_in=256, ch_out=128)\n",
    "        \n",
    "        self.Up2 = up_conv(ch_in=128,ch_out=64)\n",
    "        self.Up_conv2 = conv_block(ch_in=128, ch_out=64)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(64,output_ch,kernel_size=1,stride=1,padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # encoding path\n",
    "        x1 = self.Conv1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "        \n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.Conv4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.Conv5(x5)\n",
    "\n",
    "        # decoding + concat path\n",
    "        d5 = self.Up5(x5)\n",
    "        d5 = torch.cat((x4,d5),dim=1)\n",
    "        \n",
    "        d5 = self.Up_conv5(d5)\n",
    "        \n",
    "        d4 = self.Up4(d5)\n",
    "        d4 = torch.cat((x3,d4),dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        d3 = torch.cat((x2,d3),dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        d2 = torch.cat((x1,d2),dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "        d1 = self.sigmoid(d1)\n",
    "\n",
    "        return d1\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                               mode='min',\t# Loss최소화,최대화 결정\n",
    "                                                               factor=1e-8,\t# 학습률 감소율\n",
    "                                                               patience=5,\n",
    "                                                               verbose=True,)\n",
    "        monitor_metric = 'val_loss'\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': monitor_metric  # Specify the metric you want to monitor\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch['X'], batch['y']\n",
    "        outputs = self(x).squeeze(dim=1)\n",
    "        loss = nn.BCELoss()(outputs, y)\n",
    "        \n",
    "        predicted_masks = (outputs > 0.5).to(torch.uint8) # 0아니면 1로 바꾸는\n",
    "        y = y.to(torch.uint8)\n",
    "        \n",
    "        iou = self.jaccard(predicted_masks, y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        self.log('train_iou', iou, on_step=True, on_epoch=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch['X'], batch['y']\n",
    "        outputs = self(x).squeeze(dim=1)\n",
    "        loss = nn.BCELoss()(outputs, y)\n",
    "        \n",
    "        predicted_masks = (outputs > 0.5).to(torch.uint8)\n",
    "        y = y.to(torch.uint8)\n",
    "  \n",
    "        iou = self.jaccard(predicted_masks, y)\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_iou', iou, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class R2U_Net(pl.LightningModule):\n",
    "    def __init__(self,img_ch=3,output_ch=1,t=2):\n",
    "        super(R2U_Net,self).__init__()\n",
    "        self.jaccard = BinaryJaccardIndex()\n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.Upsample = nn.Upsample(scale_factor=2)\n",
    "\n",
    "        self.RRCNN1 = RRCNN_block(ch_in=img_ch,ch_out=64,t=t)\n",
    "\n",
    "        self.RRCNN2 = RRCNN_block(ch_in=64,ch_out=128,t=t)\n",
    "        \n",
    "        self.RRCNN3 = RRCNN_block(ch_in=128,ch_out=256,t=t)\n",
    "        \n",
    "        self.RRCNN4 = RRCNN_block(ch_in=256,ch_out=512,t=t)\n",
    "        \n",
    "        self.RRCNN5 = RRCNN_block(ch_in=512,ch_out=1024,t=t)\n",
    "        \n",
    "\n",
    "        self.Up5 = up_conv(ch_in=1024,ch_out=512)\n",
    "        self.Up_RRCNN5 = RRCNN_block(ch_in=1024, ch_out=512,t=t)\n",
    "        \n",
    "        self.Up4 = up_conv(ch_in=512,ch_out=256)\n",
    "        self.Up_RRCNN4 = RRCNN_block(ch_in=512, ch_out=256,t=t)\n",
    "        \n",
    "        self.Up3 = up_conv(ch_in=256,ch_out=128)\n",
    "        self.Up_RRCNN3 = RRCNN_block(ch_in=256, ch_out=128,t=t)\n",
    "        \n",
    "        self.Up2 = up_conv(ch_in=128,ch_out=64)\n",
    "        self.Up_RRCNN2 = RRCNN_block(ch_in=128, ch_out=64,t=t)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(64,output_ch,kernel_size=1,stride=1,padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # encoding path\n",
    "        x1 = self.RRCNN1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.RRCNN2(x2)\n",
    "        \n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.RRCNN3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.RRCNN4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.RRCNN5(x5)\n",
    "\n",
    "        # decoding + concat path\n",
    "        d5 = self.Up5(x5)\n",
    "        d5 = torch.cat((x4,d5),dim=1)\n",
    "        d5 = self.Up_RRCNN5(d5)\n",
    "        \n",
    "        d4 = self.Up4(d5)\n",
    "        d4 = torch.cat((x3,d4),dim=1)\n",
    "        d4 = self.Up_RRCNN4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        d3 = torch.cat((x2,d3),dim=1)\n",
    "        d3 = self.Up_RRCNN3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        d2 = torch.cat((x1,d2),dim=1)\n",
    "        d2 = self.Up_RRCNN2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "        d1 = self.sigmoid(d1)\n",
    "\n",
    "        return d1\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                               mode='min',\t# Loss최소화,최대화 결정\n",
    "                                                               factor=1e-8,\t# 학습률 감소율\n",
    "                                                               patience=5,\n",
    "                                                               verbose=True,)\n",
    "        monitor_metric = 'val_loss'\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': monitor_metric  # Specify the metric you want to monitor\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch['X'], batch['y']\n",
    "        outputs = self(x).squeeze(dim=1)\n",
    "        loss = nn.BCELoss()(outputs, y)\n",
    "        \n",
    "        predicted_masks = (outputs > 0.5).to(torch.uint8) # 0아니면 1로 바꾸는\n",
    "        y = y.to(torch.uint8)\n",
    "        \n",
    "        iou = self.jaccard(predicted_masks, y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        self.log('train_iou', iou, on_step=True, on_epoch=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch['X'], batch['y']\n",
    "        outputs = self(x).squeeze(dim=1)\n",
    "        loss = nn.BCELoss()(outputs, y)\n",
    "        \n",
    "        predicted_masks = (outputs > 0.5).to(torch.uint8)\n",
    "        y = y.to(torch.uint8)\n",
    "  \n",
    "        iou = self.jaccard(predicted_masks, y)\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_iou', iou, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "\n",
    "class AttU_Net(pl.LightningModule):\n",
    "    def __init__(self,img_ch=3,output_ch=1):\n",
    "        super(AttU_Net,self).__init__()\n",
    "        self.jaccard = BinaryJaccardIndex()\n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(ch_in=img_ch,ch_out=64)\n",
    "        self.Conv2 = conv_block(ch_in=64,ch_out=128)\n",
    "        self.Conv3 = conv_block(ch_in=128,ch_out=256)\n",
    "        self.Conv4 = conv_block(ch_in=256,ch_out=512)\n",
    "        self.Conv5 = conv_block(ch_in=512,ch_out=1024)\n",
    "\n",
    "        self.Up5 = up_conv(ch_in=1024,ch_out=512)\n",
    "        self.Att5 = Attention_block(F_g=512,F_l=512,F_int=256)\n",
    "        self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)\n",
    "\n",
    "        self.Up4 = up_conv(ch_in=512,ch_out=256)\n",
    "        self.Att4 = Attention_block(F_g=256,F_l=256,F_int=128)\n",
    "        self.Up_conv4 = conv_block(ch_in=512, ch_out=256)\n",
    "        \n",
    "        self.Up3 = up_conv(ch_in=256,ch_out=128)\n",
    "        self.Att3 = Attention_block(F_g=128,F_l=128,F_int=64)\n",
    "        self.Up_conv3 = conv_block(ch_in=256, ch_out=128)\n",
    "        \n",
    "        self.Up2 = up_conv(ch_in=128,ch_out=64)\n",
    "        self.Att2 = Attention_block(F_g=64,F_l=64,F_int=32)\n",
    "        self.Up_conv2 = conv_block(ch_in=128, ch_out=64)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(64,output_ch,kernel_size=1,stride=1,padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # encoding path\n",
    "        x1 = self.Conv1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "        \n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.Conv4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.Conv5(x5)\n",
    "\n",
    "        # decoding + concat path\n",
    "        d5 = self.Up5(x5)\n",
    "        x4 = self.Att5(g=d5,x=x4)\n",
    "        d5 = torch.cat((x4,d5),dim=1)        \n",
    "        d5 = self.Up_conv5(d5)\n",
    "        \n",
    "        d4 = self.Up4(d5)\n",
    "        x3 = self.Att4(g=d4,x=x3)\n",
    "        d4 = torch.cat((x3,d4),dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        x2 = self.Att3(g=d3,x=x2)\n",
    "        d3 = torch.cat((x2,d3),dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        x1 = self.Att2(g=d2,x=x1)\n",
    "        d2 = torch.cat((x1,d2),dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "        d1 = self.sigmoid(d1)\n",
    "\n",
    "        return d1\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                               mode='min',\t# Loss최소화,최대화 결정\n",
    "                                                               factor=1e-8,\t# 학습률 감소율\n",
    "                                                               patience=5,\n",
    "                                                               verbose=True,)\n",
    "        monitor_metric = 'val_loss'\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': monitor_metric  # Specify the metric you want to monitor\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch['X'], batch['y']\n",
    "        outputs = self(x).squeeze(dim=1)\n",
    "        loss = nn.BCELoss()(outputs, y)\n",
    "        \n",
    "        predicted_masks = (outputs > 0.5).to(torch.uint8) # 0아니면 1로 바꾸는\n",
    "        y = y.to(torch.uint8)\n",
    "        \n",
    "        iou = self.jaccard(predicted_masks, y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        self.log('train_iou', iou, on_step=True, on_epoch=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch['X'], batch['y']\n",
    "        outputs = self(x).squeeze(dim=1)\n",
    "        loss = nn.BCELoss()(outputs, y)\n",
    "        \n",
    "        predicted_masks = (outputs > 0.5).to(torch.uint8)\n",
    "        y = y.to(torch.uint8)\n",
    "  \n",
    "        iou = self.jaccard(predicted_masks, y)\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_iou', iou, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "class R2AttU_Net(pl.LightningModule):\n",
    "    def __init__(self,img_ch=3,output_ch=1,t=2):\n",
    "        super(R2AttU_Net,self).__init__()\n",
    "        self.jaccard = BinaryJaccardIndex()\n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "        self.Upsample = nn.Upsample(scale_factor=2)\n",
    "\n",
    "        self.RRCNN1 = RRCNN_block(ch_in=img_ch,ch_out=64,t=t)\n",
    "\n",
    "        self.RRCNN2 = RRCNN_block(ch_in=64,ch_out=128,t=t)\n",
    "        \n",
    "        self.RRCNN3 = RRCNN_block(ch_in=128,ch_out=256,t=t)\n",
    "        \n",
    "        self.RRCNN4 = RRCNN_block(ch_in=256,ch_out=512,t=t)\n",
    "        \n",
    "        self.RRCNN5 = RRCNN_block(ch_in=512,ch_out=1024,t=t)\n",
    "        \n",
    "\n",
    "        self.Up5 = up_conv(ch_in=1024,ch_out=512)\n",
    "        self.Att5 = Attention_block(F_g=512,F_l=512,F_int=256)\n",
    "        self.Up_RRCNN5 = RRCNN_block(ch_in=1024, ch_out=512,t=t)\n",
    "        \n",
    "        self.Up4 = up_conv(ch_in=512,ch_out=256)\n",
    "        self.Att4 = Attention_block(F_g=256,F_l=256,F_int=128)\n",
    "        self.Up_RRCNN4 = RRCNN_block(ch_in=512, ch_out=256,t=t)\n",
    "        \n",
    "        self.Up3 = up_conv(ch_in=256,ch_out=128)\n",
    "        self.Att3 = Attention_block(F_g=128,F_l=128,F_int=64)\n",
    "        self.Up_RRCNN3 = RRCNN_block(ch_in=256, ch_out=128,t=t)\n",
    "        \n",
    "        self.Up2 = up_conv(ch_in=128,ch_out=64)\n",
    "        self.Att2 = Attention_block(F_g=64,F_l=64,F_int=32)\n",
    "        self.Up_RRCNN2 = RRCNN_block(ch_in=128, ch_out=64,t=t)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(64,output_ch,kernel_size=1,stride=1,padding=0)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # encoding path\n",
    "        x1 = self.RRCNN1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.RRCNN2(x2)\n",
    "        \n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.RRCNN3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.RRCNN4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.RRCNN5(x5)\n",
    "\n",
    "        # decoding + concat path\n",
    "        d5 = self.Up5(x5)\n",
    "        x4 = self.Att5(g=d5,x=x4)\n",
    "        d5 = torch.cat((x4,d5),dim=1)\n",
    "        d5 = self.Up_RRCNN5(d5)\n",
    "        \n",
    "        d4 = self.Up4(d5)\n",
    "        x3 = self.Att4(g=d4,x=x3)\n",
    "        d4 = torch.cat((x3,d4),dim=1)\n",
    "        d4 = self.Up_RRCNN4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        x2 = self.Att3(g=d3,x=x2)\n",
    "        d3 = torch.cat((x2,d3),dim=1)\n",
    "        d3 = self.Up_RRCNN3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        x1 = self.Att2(g=d2,x=x1)\n",
    "        d2 = torch.cat((x1,d2),dim=1)\n",
    "        d2 = self.Up_RRCNN2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "        d1 = self.sigmoid(d1)\n",
    "\n",
    "        return d1\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                               mode='min',\t# Loss최소화,최대화 결정\n",
    "                                                               factor=1e-8,\t# 학습률 감소율\n",
    "                                                               patience=5,\n",
    "                                                               verbose=True,)\n",
    "        monitor_metric = 'val_loss'\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': monitor_metric  # Specify the metric you want to monitor\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch['X'], batch['y']\n",
    "        outputs = self(x).squeeze(dim=1)\n",
    "        loss = nn.BCELoss()(outputs, y)\n",
    "        \n",
    "        predicted_masks = (outputs > 0.5).to(torch.uint8) # 0아니면 1로 바꾸는\n",
    "        y = y.to(torch.uint8)\n",
    "        \n",
    "        iou = self.jaccard(predicted_masks, y)\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        self.log('train_iou', iou, on_step=True, on_epoch=True, logger=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch['X'], batch['y']\n",
    "        outputs = self(x).squeeze(dim=1)\n",
    "        loss = nn.BCELoss()(outputs, y)\n",
    "        \n",
    "        predicted_masks = (outputs > 0.5).to(torch.uint8)\n",
    "        y = y.to(torch.uint8)\n",
    "  \n",
    "        iou = self.jaccard(predicted_masks, y)\n",
    "\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_iou', iou, prog_bar=True)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0454dd6a-dd09-4839-9f63-9ca3db608e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchan4im\u001b[0m (\u001b[33mhcim\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA RTX A6000') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20231109_205548-t4s8b8jj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hcim/AISW/runs/t4s8b8jj' target=\"_blank\">R2U_Net</a></strong> to <a href='https://wandb.ai/hcim/AISW' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hcim/AISW' target=\"_blank\">https://wandb.ai/hcim/AISW</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hcim/AISW/runs/t4s8b8jj' target=\"_blank\">https://wandb.ai/hcim/AISW/runs/t4s8b8jj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
      "\n",
      "   | Name      | Type               | Params\n",
      "--------------------------------------------------\n",
      "0  | jaccard   | BinaryJaccardIndex | 0     \n",
      "1  | Maxpool   | MaxPool2d          | 0     \n",
      "2  | Upsample  | Upsample           | 0     \n",
      "3  | RRCNN1    | RRCNN_block        | 74.4 K\n",
      "4  | RRCNN2    | RRCNN_block        | 304 K \n",
      "5  | RRCNN3    | RRCNN_block        | 1.2 M \n",
      "6  | RRCNN4    | RRCNN_block        | 4.9 M \n",
      "7  | RRCNN5    | RRCNN_block        | 19.4 M\n",
      "8  | Up5       | up_conv            | 4.7 M \n",
      "9  | Up_RRCNN5 | RRCNN_block        | 5.2 M \n",
      "10 | Up4       | up_conv            | 1.2 M \n",
      "11 | Up_RRCNN4 | RRCNN_block        | 1.3 M \n",
      "12 | Up3       | up_conv            | 295 K \n",
      "13 | Up_RRCNN3 | RRCNN_block        | 328 K \n",
      "14 | Up2       | up_conv            | 73.9 K\n",
      "15 | Up_RRCNN2 | RRCNN_block        | 82.4 K\n",
      "16 | Conv_1x1  | Conv2d             | 65    \n",
      "17 | sigmoid   | Sigmoid            | 0     \n",
      "--------------------------------------------------\n",
      "39.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "39.1 M    Total params\n",
      "156.366   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17cca6b5337495b881bbad22cc3fccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.146\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "# WandbLogger를 초기화\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "wandb.login(key='eed81e1c0a41dd8dd67a4ca90cea1be5a06d4eb0')\n",
    "wandb_logger = WandbLogger(project='AISW', entity='hcim', name='R2U_Net')\n",
    "\n",
    "# U_Net , R2U_Net , AttU_Net , R2AttU_Net\n",
    "model = R2U_Net()\n",
    "\n",
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=8,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(devices=[0],max_epochs=100, logger=wandb_logger, callbacks=[early_stopping_callback])\n",
    "trainer.fit(model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ade64b-5f57-4fd4-b570-9ac0e999f58e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1da3772e-2933-49a9-896f-1a63037e4206",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass segDataset(Dataset):\\n    def __init__(self, data_path, transforms=None):\\n        # cata path 설정 잘하기\\n        self.pos_imgs = sorted(glob(data_path + 'Positive/Image/*'))\\n        self.pos_labels = sorted(glob(data_path + 'Positive/Label/*'))\\n        self.neg_imgs = sorted(glob(data_path + 'Negative/Image/*'))\\n        self.neg_labels = sorted(glob(data_path + 'Negative/Label /*'))\\n        # positive와 negative를 합쳐서 불러오는 코드를 작성\\n        self.imgs = self.pos_imgs + self.neg_imgs\\n        self.labels = self.pos_labels + self.neg_labels\\n        self.transforms = transforms\\n        \\n    def __len__(self):\\n        return len(self.labels)\\n        \\n    def __getitem__(self, item):\\n        img_path = self.imgs[item]\\n        label_path = self.labels[item]\\n        img = cv2.imread(img_path)\\n        label = cv2.imread(label_path, cv2.IMREAD_UNCHANGED)\\n        label = np.expand_dims(label, axis=2)\\n\\n        concat = np.concatenate([img, label], axis=2)\\n        concat = torch.from_numpy(concat)\\n        concat = concat.permute(2,0,1) # (h,w,c -> c,h,w)\\n\\n        \\n        if self.transforms:\\n            imgs = self.transforms(concat)\\n\\n        X = imgs[:3].to(torch.float32)\\n        y = imgs[3].to(torch.float32)\\n            \\n        return {'X' : X/255, 'y': y/255}\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class segDataset(Dataset):\n",
    "    def __init__(self, data_path, transforms=None):\n",
    "        # cata path 설정 잘하기\n",
    "        self.pos_imgs = sorted(glob(data_path + 'Positive/Image/*'))\n",
    "        self.pos_labels = sorted(glob(data_path + 'Positive/Label/*'))\n",
    "        self.neg_imgs = sorted(glob(data_path + 'Negative/Image/*'))\n",
    "        self.neg_labels = sorted(glob(data_path + 'Negative/Label /*'))\n",
    "        # positive와 negative를 합쳐서 불러오는 코드를 작성\n",
    "        self.imgs = self.pos_imgs + self.neg_imgs\n",
    "        self.labels = self.pos_labels + self.neg_labels\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        img_path = self.imgs[item]\n",
    "        label_path = self.labels[item]\n",
    "        img = cv2.imread(img_path)\n",
    "        label = cv2.imread(label_path, cv2.IMREAD_UNCHANGED)\n",
    "        label = np.expand_dims(label, axis=2)\n",
    "\n",
    "        concat = np.concatenate([img, label], axis=2)\n",
    "        concat = torch.from_numpy(concat)\n",
    "        concat = concat.permute(2,0,1) # (h,w,c -> c,h,w)\n",
    "\n",
    "        \n",
    "        if self.transforms:\n",
    "            imgs = self.transforms(concat)\n",
    "\n",
    "        X = imgs[:3].to(torch.float32)\n",
    "        y = imgs[3].to(torch.float32)\n",
    "            \n",
    "        return {'X' : X/255, 'y': y/255}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "906a64f9-830a-4710-9c55-a77a8d8888c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "data_path = './data/Train/'\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "set_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39f9c60c-e94a-4bec-be26-574e44446e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor.squeeze()\n",
    "    numpy_image = tensor.cpu().numpy()\n",
    "    numpy_image = (numpy_image * 255).astype(np.uint8)\n",
    "    \n",
    "    if numpy_image.shape[0] == 1: # 흑백 이미지라면 채널 차원을 제거.\n",
    "        numpy_image = np.squeeze(numpy_image, axis=0)\n",
    "        \n",
    "    elif numpy_image.shape[0] == 3: # RGB 이미지라면 채널 차원을 맨 뒤로 이동.\n",
    "        numpy_image = np.transpose(numpy_image, (1, 2, 0))\n",
    "    return numpy_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07cbdf2c-5b2c-4f25-9300-2437bac6ed9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "model.eval()\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(), # (h,w,c -> c,h,w) + Normalize\n",
    "])\n",
    "def inference_and_save_images(input_dir, output_dir, start_index, end_index):\n",
    "    for i in range(start_index, end_index + 1):\n",
    "        input_path = os.path.join(input_dir, f\"{i}.jpg\")\n",
    "        output_path = os.path.join(output_dir, f\"{i}.jpg\")\n",
    "\n",
    "        # 파일이 존재하지 않으면 스킵. Test/Positive/8322.jpg가 없음\n",
    "        if not os.path.exists(input_path):\n",
    "            print(f\"File {input_path} not found. Skipping...\")\n",
    "            continue\n",
    "            \n",
    "        # 이미지 불러오기 및 전처리\n",
    "        img = Image.open(input_path)\n",
    "        img = transform(img).unsqueeze(0)\n",
    "\n",
    "        # 추론\n",
    "        with torch.no_grad():\n",
    "            output = model(img)\n",
    "\n",
    "        # 텐서를 이미지로 변환후 저장\n",
    "        output_image = tensor_to_image(output)\n",
    "        cv2.imwrite(output_path, output_image)\n",
    "\n",
    "        print(f\"Inferred and saved {input_path} to {output_path}\")\n",
    "\n",
    "# Positive 예측\n",
    "positive_input_dir = \"./data/Test/Positive/\"\n",
    "positive_output_dir = \"./data/Prediction/Positive/\"\n",
    "inference_and_save_images(positive_input_dir, positive_output_dir, 8000, 8817)\n",
    "\n",
    "# Negative 예측\n",
    "negative_input_dir = \"./data/Test/Negative/\"\n",
    "negative_output_dir = \"./data/Prediction/Negative/\"\n",
    "inference_and_save_images(negative_input_dir, negative_output_dir, 1001, 1181)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2cbb3-b0ff-4469-9dfe-8a646ef689d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ffab85-e4ef-40e5-964e-077ef73110de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!zip -r Prediction.zip ./data/Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5c03fd-5e4b-4693-a737-8b3fcd73e9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for i in range(x.shape[0]):\n",
    "    x = x.permute(0, 2, 3, 1)\n",
    "    cv2.imwrite(f\"{batch_idx}_img.jpg\", x[i].cpu().numpy()*255)\n",
    "    cv2.imwrite(f\"{batch_idx}_label.jpg\", y[i].cpu().numpy()*255)\n",
    "    cv2.imwrite(f\"{batch_idx}_output.jpg\", outputs[i].cpu().detach().numpy()*255) \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c971b45-1f1e-4c44-b158-315c80f60623",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
